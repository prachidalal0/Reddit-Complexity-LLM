# -*- coding: utf-8 -*-
"""1_Sparse_Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ucxRHTgPJ51ABt34-DmOAMApUEZXnQD

# <font color = 'pickle'> **Bag of Words (Sparse Embeddings)**

<font color = 'pickle' size =5 >**What is Bag of Words (BoW)?**</font>

A **bag-of-words** is a representation of text that describes the occurrence of words within a document <font color ='indianred'>**disregarding grammar and word order**</font>. It involves two steps:

    1. Create Vocabulary. Each word in vocabulary forms feature(independent variable) to represent document.
    2. Score words (based on frequency or occurrence) to create Vectors.
    - sparse bc lots of 0s.
    - embeddings are the same thing as vectors
    - the easiest way to do embeddings is one hot encoding
    - the first step in vectorization --> dictionary, have to create a vocab, a mapping of a word to an index, once we have that we can see if the word is present or not --> sparse embedding
    - index value gives embedding

<font color = 'pickle' size =5> **Why do you need to learn Bag of Words?**</font>

- Till now we have learnt how to pre-process the text data i.e clean the text data.
- Our final goal is to use text data in Machine Learning (ML) models. For example - we want to predict whether e-mail is a spam or not based on the text of the data.
- But ML models can understand only numbers. Therefore we need to convert text to vectors (numbers).
- The simple method of converting text to numbers is to use 'Bag of Words approach'

<font color = 'pickle' size =5> **Why do you need Bag of Words in age of LLMs?** </font>

<font color = 'indianred'> **Outstanding paper Award ACL 2023: Linear Classifier: An Often-Forgotten Baseline for Text Classification**</font>

*Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods.*

## <font color = 'pickle' size =5 >**Learning Outcome** </font>
After completing this tutorial, you will know

1. What the bag-of-words approach is and how you can use it to represent text data.
2. What are different techniques to prepare a vocabulary and score words.
3. How to implement 'Bag-of-words' approach in python using sklearn.

# <font color = 'pickle'> **Tutorial Overview**</font>
 - Generating Vocab
 - Generating vectors using Vocab
     - Binary Vectorizer
     - Count Vectorizer
     - tfidf Vectorizer

 - Modifying Vocab
 - Example - IMDB Dataset

# <font color = 'pickle'> Import/install Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
#everytime you load the module, it imports the changes instead of importing it again

"""The code `%load_ext autoreload` and `%autoreload 2` in a Jupyter notebook enables the autoreload extension. This setup automatically reloads imported modules before executing code cells. Specifically, `%autoreload 2` ensures all modules are reloaded each time before execution, reflecting any changes made to the module files without needing to restart the notebook kernel."""

import sys

sys.path

if 'google.colab' in str(get_ipython()):
    from google.colab import drive
    drive.mount('/content/drive')

    !pip install -U nltk -qq
    !pip install -U spacy -qq
    !python -m spacy download en_core_web_sm -qq

    basepath = '/content/drive/MyDrive/data'
    sys.path.append('/content/drive/MyDrive/data/custom-functions')
else:
    basepath = '/home/harpreet/Insync/google_drive_shaannoor/data'
    sys.path.append(
    '/home/harpreet/Insync/google_drive_shaannoor/data/custom-functions')

"""**Code Explanation**

1. **Environment Detection**:
The code begins by checking the environment in which it's running. This is done using the statement `if 'google.colab' in str(get_ipython()):`. If this condition is true, it means the code is running in Google Colab, an online interactive development environment. If not, the code assumes it's running in a local environment (like your personal computer).

2. **Setup for Google Colab:**
When running in Google Colab, the code performs the following actions:

- *Mount Google Drive*: `from google.colab import drive` and `drive.mount('/content/drive')` are used to mount the user's Google Drive to the Colab environment, enabling access to files stored there.

- *Install Necessary Libraries*: The code installs or updates specific Python libraries (`nltk` and `spacy`) silently without producing unnecessary output (`-qq`).

- *Download spaCy Model*: `!python -m spacy download en_core_web_sm -qq` downloads the English language model for spaCy, necessary for NLP tasks.

- *Set Basepath and Update sys.path*:
  - `basepath` is set to a path in Google Drive where data or relevant files are stored.
  - `sys.path.append('/content/drive/MyDrive/data/custom-functions')` adds a directory to the Python search path. This allows Python to import and use custom functions located in that directory in the Colab notebook, promoting modularity and code reusability.

3. **Setup for Local Environment:**
If the code is not running in Google Colab, it's presumed to be in a local environment. Here, the setup is slightly different:

- *Set Basepath*: `basepath` is set to a specific directory on the local machine. This is where the code will look for data files or other resources.

- *Update sys.path for Custom Functions*:
  - `sys.path.append('/home/harpreet/Insync/google_drive_shaannoor/data/custom-functions')` adds a local directory to the Python search path. Similar to the Colab setup, this step allows the local Python environment to import and use custom functions from the specified directory.
"""

# Import required libraries
import pandas as pd  # For data manipulation and analysis
import numpy as np   # For numerical operations
import spacy         # For NLP preprocessing

# Import required nltk packages
import nltk
nltk.download('stopwords')  # Download the stopwords corpus
from nltk.corpus import stopwords as nltk_stopwords  # Stopwords corpus

# Import tweet tokenizer from nltk
from nltk.tokenize import TweetTokenizer

# Import CountVectorizer and TfidfVectorizer from scikit-learn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Import pathlib for managing file paths
from pathlib import Path

# import custom-preprocessor from python file
import CustomPreprocessorSpacy as cp

spacy.__version__

# load spacy model
nlp = spacy.load('en_core_web_sm')

"""## <font color = 'pickle'> **Generating Vocab**

###  <font color = 'pickle'> **Dummy Corpus**
"""

# Dummy corpus
Corpus = ["Count Vectorizer - for this vectorizer, scoring is done based on frequency. For this vectorizer frequency is key. @vectorizer #frequency @frequency, doesn’t",
          "tfidf vectorizer - for this vectorizer, scoring is done based on tfidf,  higher tfidf higher score #tfidf @vectorizer "  ,
          "Binary vectorizer - for this vectorizer, scoring is done based on presence of word. For this vectorizer, dummy is key #dummy @dummy @vectorizer "]

"""### <font color = 'pickle'>**Create an instance of Vectorizer**"""

vectorizer = CountVectorizer()
#when calling a class it calls the __init__?

"""The above code creates an instance of the `CountVectorizer` class from the `sklearn.feature_extraction.text module`. This class is used to convert a collection of text documents to a matrix of token counts.

It accomplishes this by
  1. tokenizing the input text
  2. creating a vocabulary of all the tokens found in the text
  3. encoding the text as a matrix of token counts based on this vocabulary.

The created instance vectorizer can then be used to fit the text data to the vocabulary and generate the token count matrix.
"""

CountVectorizer??

"""### <font color = 'pickle'>**Fit Vectorizer on corpus to generate vocab**"""

# Fit the vectorizer on corpus
vectorizer.fit(Corpus)

"""<font color = 'indianred'>**Vectorizer().fit() does the following**:
- lowercases your text
- uses utf-8 encoding
- performs tokenization (converts raw text to smaller units of text)
- uses word level tokenization (meaning each word is treated as a separate token) and  ignores single characters during tokenization ( words like ‘a’ and ‘I’ are removed)
- By default, the regular expression that is used to split the text and create tokens is : `"\b\w\w+\b"`.
  - This means it finds all sequences of characters that consist of at least two letters or numbers(\w) and that are separated by word boundaries (\b).
  - It does not find single-letter words, and it splits up contractions like “doesn’t” or “bit.ly”, but it matches “h8ter” as a single word.
- The CountVectorizer then converts all words to lowercasecharacters, so that “soon”, “Soon”, and “sOon” all correspond to the same token (and therefore feature).
- It then creates a dictionary of unique words.
- The set of unique words is used as features in the CountVectorizer.
- why is this better than white space? using word boundaries instead
"""

# Display the mapping of terms to feature indices created by the vectorizer
vectorizer.vocabulary_
#fit is only on the train, create vocab only based on training set

"""We can use `vectorizer.get_feature_names_out()` to obtain the list of unique words that the CountVectorizer has identified from the corpus"""

# Retrieve the list of unique words (tokens) that the CountVectorizer identified in the corpus.
# These words serve as features (columns) in the vectorized output.
features = vectorizer.get_feature_names_out()
print(features)

# Print the total number of unique features (words) identified in the corpus
print('\ntotal number of unique features (words):', len(features))

"""## <font color = 'pickle'>**Generate Vectors using Vocab**

### <font color = 'pickle'>**Binary Vectorizer**
"""

binary_vectorizer = CountVectorizer(binary=True)
binary_vectorizer.fit(Corpus)

"""- We can now call transform() method to transform documents in our corpus to vectors.
- <font color = 'dodgerblue'>**Each document**</font> will be represented by <font color = 'dodgerblue'>**vector of length equal to len(dictionary)**.</font>
- The vectors are stored in the form of a <font color = 'dodgerblue'>**sparse matrix**.</font>
- We can use <font color = 'dodgerblue'>**toarray()**</font> function to get complete matrix.
- Number of columns represent the number of features (len(vocab)).
- Number of rows represent the number the documents in a corpus.
- <font color = 'dodgerblue'>**For each row, the numbers displayed are 0 or 1 - indicating absence or presence of a word in a document.**
"""

binary_vectors = binary_vectorizer.transform(Corpus)

print(f'vectors in sparse format')
print(binary_vectors)
#not storing whole vector, only storing 1s to save space

print(f'\nbinary vectors in array(dense) format')
print(binary_vectors.toarray())
print(
    f'\nThe shape of the binary vectors is : {binary_vectors.toarray().shape}')

# create dataframe for better visualization
df_binary = pd.DataFrame(binary_vectors.toarray(), columns=features)
df_binary
#word is present or not, and frequency of the word, still not capturing meaning, order, etc.

"""### <font color = 'pickle'>**Count Vectorizer**
-  The vectors are stored in the form of a sparse matrix.
- Number of columns represent the number of features (len(vocab))
- Number of rows represent the number the documents in a corpus
- Thus, each document is represented by a vector of size of length of vocab.
- For each row, <font color = 'dodgerblue'>**the numbers displayed are the number of times a particular word has occurred in the document.**
"""

term_freq_vectorizer = CountVectorizer(binary=False)
# we can combine fit and transform steps into a single step using fit_transform()
count_vectors = term_freq_vectorizer.fit_transform(Corpus)
print(f'count vectors in array (dense) format\n')
print(count_vectors.toarray())
print(f'\nThe shape of the count vectors is : {count_vectors.toarray().shape}')

# create dataframe for better visualization
df_count = pd.DataFrame(count_vectors.toarray(),
                        columns=term_freq_vectorizer.get_feature_names_out())
df_count

"""### <font color = 'pickle'>**tf-idf Vectorizer**</font>

- One measure of how important a word is term frequency (tf) (how frequently a word occurs in a document). We examined term frequency in previous sections where we used CountVectorizer to get the freqency of each word.
- But there may be words in a document, that occur many times but these words also occur in all other documents as well.
- Therefore the word might not be a good representation of the document.
- We can account for this by  <font color = 'dodgerblue'>giving more importance to words that occur in fewer documents using inverse document frequency </font> ((# Number of documents) / (Number of documents containing the word)).
- This can be <font color = 'dodgerblue'>combined with term frequency</font> to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.
- The idea of tf-idf is to <font color = 'dodgerblue'>find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.</font>
- tf-idf gives more weight to the the words that are important (i.e., occur more frequently) in a given document, but occur rarely in other documents.
"""

tfidf_vectorizer = TfidfVectorizer()
# we can combine fit and transform steps into a single step using fit_transform()
tfidf_vectors = tfidf_vectorizer.fit_transform(Corpus)
print(f'tfidf vectors in array (dense) format\n')
print(tfidf_vectors.toarray())
print(f'\nThe shape of the tfidf vectors is : {tfidf_vectors.toarray().shape}')

# create dataframe for better visualization
df_tfidf = pd.DataFrame(tfidf_vectors.toarray(),
                        columns=tfidf_vectorizer.get_feature_names_out())
df_tfidf.round(4)

"""### <font color = 'pickle'>**Undertstanding tfidf calculations**

By default <br>
$\text{tfidf}(w, d) = \text{tf(w, d)} * \text{idf(w)}$
<br>
$\text{idf(w)} = \log\big(\frac{N + 1}{N_w + 1}\big) + 1$
<br><br>
if smooth_idf = False (default is True):
<br>
$\text{idf(w)} = \log\big(\frac{N }{N_w}\big) + 1$
<br><br>
if sublinear_tfbool = True (default is False)
<br>
$\text{tf(w, d)} = \log(\text{tf(w, d)} ) + 1$

Here:<br>
- $\text{tf}(w, d)$ is number of times word $w$ appears in document $d$
<br>
- $\text{idf}(w)$ is inverse document frequency of word $w$
- $N$ is total number of documents
- $N_w$ is number of documents that contain word w
"""

# Calculate inverse document frequency for each feature (word)
term_idf = tfidf_vectorizer.idf_
term_idf

# create dataframe for better visualization
df_idf = pd.DataFrame(term_idf, index=tfidf_vectorizer.get_feature_names_out())
df_idf.round(4).T

"""For better understanding, we will now just look at the first document and get the idf and term frequencies for each word in the first document"""

# create dataframe for tf vectors for the first document

# Create a dense numpy array from the sparse count vector for the first document
first_document_tf = count_vectors[0].toarray().ravel()

# Get the feature names for the term frequency vectors
feature_names_tf = term_freq_vectorizer.get_feature_names_out()

# Create a dataframe from the term frequency feature names and values
df_tf = pd.DataFrame({'features': feature_names_tf, 'tf': first_document_tf})
df_tf

"""Note: The `toarray` method is used to convert the sparse matrix into a dense numpy array, and `ravel` is used to flatten the resulting 2-dimensional array into a 1-dimensional array. This is necessary because pandas dataframes expect 1-dimensional arrays as values for the columns."""

# create dataframe for tfidf vectors for the first document
first_document_tfidf = tfidf_vectors[0].toarray().ravel()
feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()
df_tfidf = pd.DataFrame({'features': feature_names_tfidf,
                        'idf': term_idf, 'norm_tfidf': first_document_tfidf})

# combine dataframes

# Merge the tf and tf-idf dataframes on the 'features' column
df = pd.merge(left=df_tf, right=df_tfidf)

# Sort the combined dataframe by the 'norm_tfidf' column in descending order
df.sort_values(by=["norm_tfidf"], ascending=False, inplace=True)

df
#not accounting for the length of the document, word can appear 3 times in a paragraph or report --> normalize
#compare TFIDF across documents

"""**Observations from above results**
- words 'frequency' and 'vectorizer' occurs 4 times in the documsnt and hence term frequency is 4.
- Word 'vectorizer' occurs in every document and hence idf is 1 (log(1) + 1).
- norm_tfidf gives higher score to word 'frequency' than 'vectorizer'.
- norm_tfidf is not equal to idf * tf

Let us know understand how norm_tfidf is calculated:
"""

# calculate tfidf (without any normalization)
df['tfidf'] = df.eval('tf*idf')

# calculate tfidf - normalized
df['sq_tfidf'] = df.eval('tfidf**2')
df['norm_tfidf_manually'] = df['tfidf']/np.sqrt(df['sq_tfidf'].sum())
#vector / scalar (broadcasting)

"""## <font color = 'pickle'>**Modifying Vocab**
- important to filter words based on frequency
- and stop words

### <font color = 'pickle'>**Case sensitive**
"""

# The lowercase argument is set to False to indicate that the text should
# not be converted to lowercase before tokenizing.
# The resulting vocab may have same word in upper and lower case
vectorizer = CountVectorizer(lowercase=False)

# we can use fit_transform to use fit() and transform() in one step
vectors = vectorizer.fit_transform(Corpus)
vectorizer.vocabulary_
#vectorizer is two different words in the vocab, can be helpful in sarcasm detection etc
#downside is vocab list will increase

"""### <font color = 'pickle'>**Filtering words based on frequency**

The `max_df`, `min_df`, and `max_features` parameters in the `CountVectorizer`` class control the feature selection for the resulting term frequency (tf) vectors.

- `max_df`: This parameter sets the maximum threshold for the frequency of a term in the document collection. If a term has a document frequency (i.e., the number of documents that contain the term) higher than max_df, it will be ignored. <font color = 'dodgerblue' >**This parameter is used to filter out stop words (corpus specific) that appear in too many documents.** </font>

- min_df: This parameter sets the minimum threshold for the frequency of a term in the document collection. If a term has a document frequency lower than min_df, it will be ignored.  <font color = 'dodgerblue' >**This parameter is used to filter out rare words that appear in too few documents.**

- max_features: This parameter sets the maximum number of features (i.e., the maximum number of unique terms) that should be included in the resulting tf vectors. If the number of unique terms in the document collection is larger than max_features, the terms with the highest tf values will be kept and the others will be ignored.  <font color = 'dodgerblue' >**This parameter is used to reduce the dimensionality of the resulting tf vectors, which can help reduce the computational cost of downstream processing.**

By using the max_df, min_df, and max_features parameters, you can control the feature selection process and determine the most informative terms to include in the tf vectors.
"""

# remove rare words - remove words which appear in less than 2 documents
vectorizer = CountVectorizer(min_df=2)
vectorizer.fit(Corpus)
vectorizer.vocabulary_

# remove words which appear in more than 2 documents - remove corpus specific stop words
vectorizer = CountVectorizer(max_df=2)
vectorizer.fit(Corpus)
vectorizer.vocabulary_

# retain most frequent words only - retain top n words based on term frequency across corpus
vectorizer = CountVectorizer(max_features=5)
vectorizer.fit(Corpus)
vectorizer.vocabulary_

"""### <font color = 'pickle'>**Stop Words**
- nltk library,
- passing it through a vecorizor
"""

# We can also specify list of stopwords to countvectorizer to get the feature without stopwords

# Import libraries
nltk_stop_words = nltk_stopwords.words('english')

vectorizer = CountVectorizer(max_features=5, stop_words=nltk_stop_words)
vectorizer.fit(Corpus)
vectorizer.vocabulary_

"""### <font color = 'pickle'>**Custom Tokenizer and Preprocessor**
- fit part creates vocab, and before creating vocab we must create tokens
- uses /b --- taking any sequences of 2 or more, ignoring punctuation
- you can pass any tokenizer

#### <font color = 'pickle'>**nltk tokenizer**
"""

# We can use custom tokenizer e.g. we can use nltk tweet tokenizer to get each tokens as feature

# Create an instance of the TweetTokenizer class
tweet_tokenizer = TweetTokenizer()

# Initialize the CountVectorizer with the custom tokenizer
# only works if analyzer = 'word'
vectorizer = CountVectorizer(
    analyzer='word', tokenizer=tweet_tokenizer.tokenize)

vectorizer.fit_transform(Corpus)
vectorizer.vocabulary_

"""#### <font color = 'pickle'>**spacy pre-processor and tokenizer**"""

def spacy_preprocessor(text):

    # Create spacy object
    doc = nlp(text)

    # remove punctuations and get a list of tokens
    filtered_text = [token.text for token in doc if not token.is_punct]

    # join the processed tokens in to string
    return " ".join(filtered_text)

# Spacy Tokenizer
def spacy_tokenizer(data):
    doc = nlp(data)
    return [token.text for token in doc]

# custom preprocessor and spacy tokenizer
# count vectorizer?
# if we want to keep hashtag as a separate token, we can add capital T??
vectorizer = CountVectorizer(analyzer='word', preprocessor=spacy_preprocessor,
                             tokenizer=spacy_tokenizer, token_pattern=None)
vectors = vectorizer.fit(Corpus)
vectorizer.vocabulary_

"""#### <font color = 'pickle'>**custom preprocessor we created earlier**"""

cp.SpacyPreprocessor??

custom_preprocessor = cp.SpacyPreprocessor('en_core_web_sm')

#cleaning text based on values, then also doing tokenization
#join with white space token
#create preprocessor function
def spacy_preprocessor(text):
    filtered_text = custom_preprocessor.transform([text])
    return " ".join(filtered_text)

# custom preprocessor and spacy tokenizer
# passing through preprocessor
# [\S] anything that is not space is a separate token
vectorizer = CountVectorizer(analyzer='word', preprocessor=spacy_preprocessor,
                            token_pattern=r"[\S]+")
vectors = vectorizer.fit(Corpus)
vectorizer.vocabulary_

"""#### <font color = 'pickle'>**token patterns with regular expressions**"""

# We can pass regex to the argument token_pattern to get required pattern
# whitespace tokenizer
# This can be very useful if we have allready cleaned the text
#don't make preprocessing part of vectorizor
#save cleaned text as a separate part in the data
vectorizer = CountVectorizer(analyzer='word', token_pattern=r"[\S]+")

# Assign the encoded(transformed) vectors to a variable
vectors = vectorizer.fit_transform(Corpus)

vectorizer.vocabulary_

"""### <font color = 'pickle'>**ngrams**</font>

- Till now our features consists of single token. However, in some cases we may want to use sequence of tokens as features
- Consider the following corpus
 1. This item is good
 2. This item is not good
- Now  both the documents will have feature 'good' and 'not' will be an additional feature in document 2.
- For applications like sentiment analysis - it might be a good idea to consider 'not good' as a single token.

- We can use ngram_range(min_n, max_n) in CountVectorizer to create features that consists of sequence of words.

- if we specify min_n = 2 and max_n = 3, we will get bigrams and trigrams as features.
"""

min_n = 2
max_n = 2

#if min & max are different then we have bigram? unigram?
#bigram "not good" means not good
#unigram "not good" means negative?

# only works if analyzer = 'word'
vectorizer1 = CountVectorizer(analyzer='word', ngram_range=(min_n, max_n))
vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(min_n, max_n))

text1 = ["This item is not good"]
text2 = ["This item is terribly good"]

# Fit the vectorizer to text
vectorizer1.fit_transform(text1)
vectorizer2.fit_transform(text2)

features1 = vectorizer1.get_feature_names_out()
features2 = vectorizer2.get_feature_names_out()

print('Features for text 1\n')
for feature in features1:
    print(feature)

print(f'\nFeatures for text 2\n')
for feature in features2:
    print(feature)

"""## <font color = 'pickle'>**Example : IMDB Data set**

### <font color = 'pickle'>**Import Data**
"""

# Use train.csv of IMDB movie review data (we downloaded this in the last lecture)
base_folder = Path(basepath)
data_folder = base_folder/'datasets'
train_data = data_folder / 'aclImdb'/'train.csv'
test_data = data_folder / 'aclImdb'/'test.csv'

# Reading data
train_df = pd.read_csv(train_data, index_col=0)
test_df = pd.read_csv(test_data, index_col=0)
print(f'Shape of Training data set is : {train_df.shape}')
print(f'Shape of Test data set is : {test_df.shape}')
print(f'\nTop five rows of Training data set:\n')
train_df.head()

#not shown but create another column with the cleaned text

"""### <font color = 'pickle'>**Generating Vocab**</font>
- <font color = 'indianred'>**Vocab should be created only based on training dataset**</font>
- We will generate vocab using CountVectorizer
- <font color = 'indianred'>**Use fit_transform() on Training data set**.
- **Use only transform() on Test dataset**. This make sures that we generate vocab only based on training dataset.
"""

# Initialize vectorizer
nltk_stop_words = nltk_stopwords.words('english')
bag_of_word = CountVectorizer(stop_words=nltk_stop_words)

# Fit on training data
bag_of_word.fit(train_df['Reviews'].values)

# get feature names
features = bag_of_word.get_feature_names_out()

# check the legth of the vocab
len(features)
#too many tokens --> limit vocab

"""### <font color = 'pickle'>**Create vectors for reviews**"""

# Transform the training and test dataset
bow_vector_train = bag_of_word.transform(train_df['Reviews'].values)
bow_vector_test = bag_of_word.transform(test_df['Reviews'].values)

# Shape of the matrix for train dataset
bow_vector_train

# Shape of the matrix for test dataset
bow_vector_test

"""### <font color = 'pickle'>**Limit vocab using max_features**
We got 25k rows with 78k+ features, but what if we want only top 5k features.
We can do this by providing max_features parameter.
"""

# Limit Vocab size using Max features
spacy_stop_words = nlp.Defaults.stop_words
bag_of_word = CountVectorizer(
    max_features=5000, stop_words=list(spacy_stop_words))  # Max features

# Fit on training data
bag_of_word.fit(train_df['Reviews'].values)

# Transform the training and test dataset
# not doing fit on the test --> or else it has already seen the data
# dont have to worry about that in our preprocessor because the test set was empty
# fit on train and transform on test
bow_vector_train = bag_of_word.transform(train_df['Reviews'].values)
bow_vector_test = bag_of_word.transform(train_df['Reviews'].values)

# Document representation
vocab = bag_of_word.get_feature_names_out()
pd.DataFrame(bow_vector_train.toarray(), columns=vocab)