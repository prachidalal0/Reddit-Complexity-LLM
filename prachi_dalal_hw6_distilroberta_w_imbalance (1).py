# -*- coding: utf-8 -*-
"""Prachi_Dalal_HW6_DistilRoberta_w/Imbalance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w9BPD4r2T3u0xhhTU_D132tEQ-oJENp7

# <Font color = 'indianred'>**HW6 - Prachi Dalal - Updated**

I had submitted my original code, however, given that my code was not performing well, I updated my code to align with the dataset submitted in the Kaggle Competition. This is the updated code.

# <Font color = 'indianred'>**Set Environment**
"""

# If in Colab, then import the drive module from google.colab
if 'google.colab' in str(get_ipython()):
  from google.colab import drive
  # Mount the Google Drive to access files stored there
  drive.mount('/content/drive')
    # Install the latest version of torchtext library quietly without showing output
  !pip install torchtext -qq
  !pip install transformers evaluate wandb datasets accelerate -U -qq ## NEW LINES ##
  basepath = '/content/drive/MyDrive/BUAN 6342'
else:
  basepath = '/Users/prachidalal/Desktop/SPRING\ 2024/BUAN\ 6342'



import torch
import transformers
import datasets
import sklearn
import spacy
import numpy
import joblib
import seaborn
import matplotlib
import wandb
import bs4

print("PyTorch version:", torch.__version__)
print("Transformers version:", transformers.__version__)
print("Datasets version:", datasets.__version__)
print("Scikit-learn version:", sklearn.__version__)
print("spaCy version:", spacy.__version__)
print("NumPy version:", numpy.__version__)
print("Joblib version:", joblib.__version__)
print("Seaborn version:", seaborn.__version__)
print("Matplotlib version:", matplotlib.__version__)
print("Weights & Biases version:", wandb.__version__)
print("Beautiful Soup version:", bs4.__version__)

get_ipython()

base_folder = Path(basepath)
model_folder = base_folder/'models'

model_folder.mkdir(exist_ok=True, parents = True)
model_folder

pip install transformers datasets

pip install datasets

train_val = load_dataset('harpreetmann/train_emotion_spring_2024')

train_val

# Access the first element in the list and check its type
first_inner_element_type = type(train_val['train']['label'][0][0])
print(first_inner_element_type)

train_val['train'][0:2]

checkpoint = "distilroberta-base"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
def tokenize_fn(batch):
    return tokenizer(batch["text"], truncation=True, padding=True)

from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=11, problem_type="multi_label_classification")

tokenized_dataset= train_val.map(tokenize_fn, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(
    ['text']
)

tokenized_dataset

model

labels = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism',
 'pessimism', 'sadness', 'surprise', 'trust']

config = AutoConfig.from_pretrained(checkpoint)
id2label= {id_: label_ for id_, label_ in enumerate(labels)}
label2id = {label_: id_ for id_, label_ in enumerate(labels)}
config.id2label = id2label
config.label2id = label2id
model.config = config

accuracy_metric = evaluate.load('accuracy', 'multilabel')
f1 = evaluate.load('f1','multilabel')


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    # print(logits.shape)
    preds = (logits > 0).astype(int)
    accuracy = accuracy_metric.compute(predictions=preds, references=labels)
    f1_micro = f1.compute(predictions=preds, references=labels, average='micro')
    f1_macro = f1.compute(predictions=preds, references=labels, average='macro')
    return {'f1_micro':f1_micro['f1'],
            'f1_macro':f1_macro['f1'],
            'accuracy':accuracy['accuracy'],
            }

# Configure training arguments
training_args = TrainingArguments(
    output_dir=str(model_folder),
    per_device_train_batch_size=128,
    per_device_eval_batch_size=128,
    weight_decay=0.01,
    learning_rate=2e-5,
    lr_scheduler_type='linear',
    warmup_steps=0,  # Number of warmup steps for the learning rate scheduler
    optim='adamw_torch',
    num_train_epochs=10,
    evaluation_strategy='steps',
    eval_steps=20,
    save_strategy="steps",
    save_steps=20,
    load_best_model_at_end=True,
    save_total_limit=2,
    metric_for_best_model="eval_f1_macro",
    greater_is_better=True,
    logging_strategy='steps',
    logging_steps=20,
    report_to='wandb',
    run_name='tweet_hf_trainer',
    fp16=True,
)

#class imbalance
def calculate_pos_weights(dataset):
    # Initialize counters for all labels
    num_labels = len(dataset['train']['label'][0])
    total_positives = [0] * num_labels
    total_negatives = [0] * num_labels

    # Count positives and negatives for each label
    for label_array in dataset['train']['label']:
        for i, label in enumerate(label_array):
            if label == 1:
                total_positives[i] += 1
            else:
                total_negatives[i] += 1

    # Calculate pos_weight for each label
    pos_weight = [total_negatives[i] / max(total_positives[i], 1) for i in range(num_labels)]
    return torch.tensor(pos_weight)

# Calculate the pos_weight using the training set
pos_weights = calculate_pos_weights(train_val)

pos_weights

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        # print(inputs)
        # Extract labels and remove them from inputs
        labels = inputs.pop("labels").float()  # Ensure labels are float for BCE loss
        # print(labels)
        outputs = model(**inputs)
        logits = outputs.get("logits")

        device = next(model.parameters()).device

        # Compute custom loss (BCEWithLogitsLoss is suitable for multi-label)
        # pos_weight can be used to handle class imbalance
        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weights.to(device))
        # Reshape labels to match logits dimensions
        loss = loss_fct(logits, labels)

        return (loss, outputs) if return_outputs else loss

trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["valid"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,

)

!wandb login

# Commented out IPython magic to ensure Python compatibility.
# %env WANDB_PROJECT = nlp_course_spring_2024-sentiment-analysis-hf-trainer

trainer.train()

trainer.evaluate()

eval_output = trainer.evaluate(tokenized_dataset["valid"])

eval_output

valid_output = trainer.predict(tokenized_dataset["valid"])
valid_preds = (valid_output.predictions > 0).astype(int)
valid_labels = valid_output.label_ids.astype(int)

from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, f1_score
y_true = valid_labels
y_pred = valid_preds
class_names = labels

mcm = multilabel_confusion_matrix(y_true, y_pred,)

# 1. Individual Heatmaps
for idx, matrix in enumerate(mcm):
    plt.figure(figsize=(5, 4))
    sns.heatmap(matrix, annot=True, fmt='g', cmap='Blues',
                xticklabels=['Predicted Negative', 'Predicted Positive'],
                yticklabels=['True Negative', 'True Positive'])
    plt.title(f'Confusion Matrix for {class_names[idx]}')
    plt.show()

# 2. Aggregate Metrics Heatmap
precision_per_class = precision_score(y_true, y_pred, average=None)
recall_per_class = recall_score(y_true, y_pred, average=None)
f1_per_class = f1_score(y_true, y_pred, average=None)

metrics_df = pd.DataFrame({
    'Precision': precision_per_class,
    'Recall': recall_per_class,
    'F1-Score': f1_per_class
}, index=class_names)

plt.figure(figsize=(10, 8))
# sns.heatmap(metrics_df, annot=True, cmap='Blues')
# plt.title('Metrics for each class')
# plt.show()

ax = sns.heatmap(metrics_df, annot=True, cmap='Blues')
plt.title('Metrics for each class')
plt.tight_layout()  # Adjust layout to not cut off edges

# Log the heatmap to wandb
wandb.log({"Metrics Heatmap": wandb.Image(ax.get_figure())})
plt.show()

# 3. Histogram of Metrics
metrics_df.plot(kind='bar', figsize=(12, 7))
plt.ylabel('Score')
plt.title('Precision, Recall, and F1-Score for Each Class')
plt.show()

wandb.finish()

print("Validation Macro F1 Score: ", f1_score(valid_labels,valid_preds, average= 'macro'), "\nValidation Hamming Loss: ",hamming_loss(valid_labels,valid_preds))

best_model_checkpoint_step = trainer.state.best_model_checkpoint.split('-')[-1]
print(f"The best model was saved at step {best_model_checkpoint_step}.")

# Read the CSV file into a pandas DataFrame
path = "/content/drive/MyDrive/BUAN 6342/test.csv"
df_test = pd.read_csv(path)

df_test.head()

# Replace "NONE" labels with 0s
label_columns = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']
df_test[label_columns] = df_test[label_columns].replace('NONE', 0)

# Verify the changes
df_test.head()

import torch
X_test = df_test['Tweet'].tolist()
y_test = df_test[label_columns].values.astype(float).tolist()
y_test = torch.tensor(y_test)
test_split = Dataset.from_dict({
    'text': X_test,
    'labels': y_test
})

test_split

test_set_tokenized = test_split.map(tokenize_fn, batched=True)

predictions = trainer.predict(test_set_tokenized)
predicted_labels = (predictions.predictions > 0).astype(int)

import pandas as pd

predicted_data = {'ID': df_test['ID']}

for i, label in enumerate(label_columns):
    predicted_data[f'{label}'] = predicted_labels[:, i]

predicted_df = pd.DataFrame(predicted_data)

predicted_df

# Specify the directory where you want to save the file
basepath = '/content/drive/MyDrive/BUAN 6342/'

# Concatenate the basepath with the desired filename
csv_file_path = basepath + 'test_kaggle.csv'

# Save the DataFrame to CSV at the specified path
predicted_df.to_csv(csv_file_path, index=False)

print("CSV file saved to:", csv_file_path)

