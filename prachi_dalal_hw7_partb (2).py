# -*- coding: utf-8 -*-
"""Prachi_Dalal_HW7_PartB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XJPfMzktDVfyXxDXlOx6j-hTknAh0T-e

# <Font color = 'indianred'>**HW7 Part B- Prachi Dalal**
"""

# If in Colab, then import the drive module from google.colab
if 'google.colab' in str(get_ipython()):
  from google.colab import drive
  # Mount the Google Drive to access files stored there
  drive.mount('/content/drive')
    # Install the latest version of torchtext library quietly without showing output
  !pip install transformers evaluate wandb datasets accelerate peft bitsandbytes -U -qq ## NEW LINES ##
  basepath = '/content/drive/MyDrive/BUAN 6342'
else:
  basepath = '/Users/prachidalal/Desktop/SPRING\ 2024/BUAN\ 6342'

# standard data science librraies for data handling and v isualization
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import numpy as np

import torch
import torch.nn as nn
from datasets import load_dataset
from transformers import (
    TrainingArguments,
    Trainer,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoConfig,
    BitsAndBytesConfig,
)

import wandb
import evaluate
import peft
from peft import (
    prepare_model_for_kbit_training,
    get_peft_model,
)

pip install transformers datasets

pip install datasets

base_folder = Path(basepath)
model_folder = base_folder/'models'

model_folder.mkdir(exist_ok=True, parents = True)
model_folder

train_val = load_dataset('harpreetmann/train_emotion_spring_2024')

train_val

train_val['train'][0:2]

labels = ['anger',
 'anticipation',
 'disgust',
 'fear',
 'joy',
 'love',
 'optimism',
 'pessimism',
 'sadness',
 'surprise',
 'trust']

from huggingface_hub import notebook_login
notebook_login()

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

checkpoint = "google/gemma-1.1-2b-it"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_fn(batch):
    return tokenizer(text = batch["text"], truncation=True)

tokenized_dataset= train_val.map(tokenize_fn, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(
    ['text']
)
# tokenized_dataset.set_format(type='torch')

tokenized_dataset



accuracy_metric = evaluate.load('accuracy', 'multilabel')
f1 = evaluate.load('f1','multilabel')

import torch

def compute_metrics(eval_pred):
    logits, labels = eval_pred

    # Convert logits to a PyTorch tensor
    logits_tensor = torch.tensor(logits)

    threshold = 0
    preds = (logits_tensor > threshold).float()

    # Ensure logits have the same shape as labels
    if logits_tensor.shape[1] != labels.shape[1]:
        raise ValueError(f"Mismatch in the number of classes between logits and labels")

    # Reshape logits if needed
    if logits_tensor.shape[0] != labels.shape[0]:
        logits_tensor = logits_tensor.transpose(1, 0)  # Transpose logits to match labels shape

    accuracy = accuracy_metric.compute(predictions=preds, references=labels)
    f1_micro = f1.compute(predictions=preds, references=labels, average='micro')
    f1_macro = f1.compute(predictions=preds, references=labels, average='macro')

    return {
        'f1_micro': f1_micro['f1'],
        'f1_macro': f1_macro['f1'],
        'accuracy': accuracy['accuracy'],
    }

# Define the directory where model checkpoints will be saved
run_name = "emotions_gemma_ia3_im"
base_folder = Path(basepath)
model_folder = base_folder / "models"/run_name
# Create the directory if it doesn't exist
model_folder.mkdir(exist_ok=True, parents=True)

# Configure training parameters
training_args = TrainingArguments(
    # Training-specific configurations
    num_train_epochs=3,  # Total number of training epochs
    # Number of samples per training batch for each device
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=8,

    weight_decay=0,  # Apply L2 regularization to prevent overfitting
    learning_rate=5e-6,  # Step size for the optimizer during training
    lr_scheduler_type='linear',
    warmup_steps=0,  # Number of warmup steps for the learning rate scheduler
    optim='adamw_torch',  # Optimizer,
    max_grad_norm = 1.0,

    # Checkpoint saving and model evaluation settings
    output_dir=str(model_folder),  # Directory to save model checkpoints
    evaluation_strategy='steps',  # Evaluate model at specified step intervals
    eval_steps=20,  # Perform evaluation every 10 training steps
    save_strategy="steps",  # Save model checkpoint at specified step intervals
    save_steps=20,  # Save a model checkpoint every 10 training steps
    load_best_model_at_end=True,  # Reload the best model at the end of training
    save_total_limit=2,  # Retain only the best and the most recent model checkpoints
    # Use 'accuracy' as the metric to determine the best model
    metric_for_best_model="eval_f1_macro",
    greater_is_better=True,  # A model is 'better' if its accuracy is higher


    # Experiment logging configurations (commented out in this example)
    logging_strategy='steps',
    logging_steps=20,
    report_to='wandb',  # Log metrics and results to Weights & Biases platform
    run_name=run_name,  # Experiment name for Weights & Biases

    #fp16=False,
    bf16=False,
    # tf32= False
)

bnb_config = BitsAndBytesConfig(
  load_in_4bit=True,
  llm_int8_skip_modules = ['score'],
  bnb_4bit_quant_type="nf4",
  bnb_4bit_use_double_quant=True,
  bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=11,problem_type="multi_label_classification", quantization_config=bnb_config)

model = prepare_model_for_kbit_training(model)
config = AutoConfig.from_pretrained(checkpoint)
id2label= {id_: label_ for id_, label_ in enumerate(labels)}
label2id = {label_: id_ for id_, label_ in enumerate(labels)}
config.id2label = id2label
config.label2id = label2id
model.config = config

model

import re
model_modules = str(model.modules)
pattern = r'\((\w+)\): Linear'
linear_layer_names = re.findall(pattern, model_modules)

names = []
# Print the names of the Linear layers
for name in linear_layer_names:
    names.append(name)
target_modules = list(set(names))
target_modules

gemma_peft_config = peft.IA3Config(task_type="SEQ_CLS",
                                    peft_type="IA3",
                                    target_modules=['score',
                                                    'o_proj',
                                                    'k_proj',
                                                    'q_proj',
                                                    'v_proj',
                                                    'down_proj',
                                                    'up_proj',
                                                    'gate_proj'],
                                    feedforward_modules=['o_proj',
                                                        'k_proj',
                                                        'q_proj',
                                                        'v_proj'],
                                    inference_mode=False)
gemma_peft_model = get_peft_model(model, gemma_peft_config)
gemma_peft_model.print_trainable_parameters()

gemma_peft_config.target_modules

gemma_peft_model

def calculate_pos_weights(dataset):
    # Initialize counters for all labels
    num_labels = len(dataset['train']['label'][0])
    total_positives = [0] * num_labels
    total_negatives = [0] * num_labels

    # Count positives and negatives for each label
    for label_array in dataset['train']['label']:
        for i, label in enumerate(label_array):
            if label == 1:
                total_positives[i] += 1
            else:
                total_negatives[i] += 1

    # Calculate pos_weight for each label
    pos_weight = [total_negatives[i] / max(total_positives[i], 1) for i in range(num_labels)]
    return torch.tensor(pos_weight)

# Calculate the pos_weight using the training set
pos_weights = calculate_pos_weights(train_val).float()

pos_weights= torch.tensor([2., 3., 2., 2., 2., 3., 2., 3., 2., 4., 4.])

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        # Retrieve labels and convert them to PyTorch tensor
        labels = inputs.pop("labels").clone().detach().float()

        # Retrieve outputs from the model
        outputs = model(**inputs)
        logits = outputs.get("logits")

        device = next(model.parameters()).device

        # Use BCEWithLogitsLoss with pos_weight
        loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weights.to(device))
        loss = loss_fct(logits, labels)

        return (loss, outputs) if return_outputs else loss

!wandb login

wandb.init(project="hw7_gemma_IA3", name="emotion_detection_gemma")

# Log your training parameters using wandb.config
wandb.config.update(training_args)

trainer = CustomTrainer(
    model=gemma_peft_model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['valid'],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.evaluate()

eval_output = trainer.evaluate(tokenized_dataset["valid"])

eval_output

wandb.log({"eval_accuracy": eval_output["eval_accuracy"], "eval_loss": eval_output["eval_loss"],
"eval_f1_micro": eval_output["eval_f1_micro"], "eval_f1_macro": eval_output["eval_f1_macro"]})

valid_output = trainer.predict(tokenized_dataset["valid"])
valid_preds = (valid_output.predictions > 0).astype(int)
valid_labels = valid_output.label_ids.astype(int)

print("Validation Macro F1 Score: ", f1_score(valid_labels,valid_preds, average= 'macro'), "\nValidation Hamming Loss: ",hamming_loss(valid_labels,valid_preds))

best_model_checkpoint_step = trainer.state.best_model_checkpoint.split('-')[-1]
print(f"The best model was saved at step {best_model_checkpoint_step}.")

wandb.finish()

# Read the CSV file into a pandas DataFrame
path = "/content/drive/MyDrive/BUAN 6342/test.csv"
df_test = pd.read_csv(path)

df_test.head()

# Replace "NONE" labels with 0s
label_columns = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']
df_test[label_columns] = df_test[label_columns].replace('NONE', 0)

# Verify the changes
df_test.head()

import torch
X_test = df_test['Tweet'].tolist()
y_test = df_test[label_columns].values.astype(float).tolist()
y_test = torch.tensor(y_test)
test_split = Dataset.from_dict({
    'text': X_test,
    'labels': y_test
})

test_split

test_set_tokenized = test_split.map(tokenize_fn, batched=True)

import pandas as pd

predicted_data = {'ID': df_test['ID']}

for i, label in enumerate(label_columns):
    predicted_data[f'{label}'] = predicted_labels[:, i]

predicted_df = pd.DataFrame(predicted_data)

predicted_df

# Specify the directory where you want to save the file
basepath = '/content/drive/MyDrive/BUAN 6342/'

# Concatenate the basepath with the desired filename
csv_file_path = basepath + 'test_kaggle.csv'

# Save the DataFrame to CSV at the specified path
predicted_df.to_csv(csv_file_path, index=False)

print("CSV file saved to:", csv_file_path)



